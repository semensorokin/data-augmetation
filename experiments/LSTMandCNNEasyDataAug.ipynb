{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMandCNNEasyDataAug.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMlrN2KAldoS8G4HlCk4cFD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/semensorokin/data_augmetation/blob/master/LSTMandCNNEasyDataAug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teOrWuQchA97",
        "colab_type": "text"
      },
      "source": [
        "models from EDA https://github.com/jasonwei20/eda_nlp/tree/d75e8bd4631f4d93260cb291aa47852d8eacd51d/experiments\n",
        "some parameters changed - size of model(lstm, CNN) layers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pUd-03e_Vrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c52bc2b-384c-45fd-95a8-dfb979ff9d32"
      },
      "source": [
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers import Bidirectional\n",
        "import keras.layers as layers\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "import keras.backend as K\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "from random import randint\n",
        "random.seed(3)\n",
        "import datetime, re, operator\n",
        "from random import shuffle\n",
        "from time import gmtime, strftime\n",
        "import gc\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join, isdir\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YgDJcEhPBF1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "31e07935-2444-4a9b-9cdc-67eb19fa27a2"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 12940136102339084745\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 12108141553435784198\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 6549464723341371551\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 7304675328\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 13292425830433952609\n",
            "physical_device_desc: \"device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qWeQ3itCxgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "01620931-680c-49bf-b83f-b270b748b224"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/'My Drive'/UniversalEmb"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/UniversalEmb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uWuFk_a6iY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm wiki-news-300d-1M-subword.vec.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n8npONbC9MO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "cdfed1c5-9a01-41d2-b11f-2c19a8bc5ea4"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "w2v_model = KeyedVectors.load_word2vec_format('wiki-news-300d-1M-subword.vec', binary=False) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl9fdZycFn-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "def read_trec(n_examples, typ = 1):\n",
        "  \"type = 0 - trec6, type = 1 - trec50\"\n",
        "  labels = [i.split(' ', 1 )[0].split(':')[typ] for i in open('train_5500.label', encoding = 'windows-1252').readlines()][:n_examples]\n",
        "  texts = [ i.split(' ', 1 )[1][:-1] for i in open('train_5500.label', encoding = 'windows-1252').readlines()][:n_examples]\n",
        "  train  = pd.DataFrame({'text':texts, 'labels': labels})\n",
        "  lbl2indx = {i:j for j, i in enumerate(train.labels.unique())}\n",
        "  train['lbl_index'] = train.labels.apply(lambda x:lbl2indx[x])\n",
        "\n",
        "  labels_t = [i.split(' ', 1)[0].split(':')[typ] for i in open('TREC_10.label', encoding = 'windows-1252').readlines()]\n",
        "  texts_t = [ i.split(' ', 1)[1][:-1] for i in open('TREC_10.label', encoding = 'windows-1252').readlines()]\n",
        "  test  = pd.DataFrame({'text':texts_t, 'labels': labels_t})\n",
        "  test['lbl_index'] = test.labels.apply(lambda x:lbl2indx[x])\n",
        "\n",
        "  return train, test, lbl2indx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-82Hj1lIMhxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(data, w2i, n_classes, input_size):\n",
        "  x_matrix = np.zeros((data.shape[0], input_size))\n",
        "  y_matrix = np.zeros((data.shape[0], n_classes))\n",
        "  sent_len = []\n",
        "  found_word, unk_w = 0,0\n",
        "  for i, line in enumerate(data.iterrows()):\n",
        "    label = line[1]['lbl_index']\n",
        "    sentence = line[1]['text']\n",
        "    splited_sent = sentence.split(' ')\n",
        "    sent_len.append(len(splited_sent))\n",
        "\n",
        "    for enum, word in enumerate(splited_sent[:input_size]):\n",
        "      if word in w2i:\n",
        "        x_matrix[i][enum] = w2i[word]\n",
        "        found_word+=1\n",
        "      else:\n",
        "        x_matrix[i][enum] = w2i['unk']\n",
        "        unk_w+=1\n",
        "    y_matrix[i][label] = 1.0\n",
        "  print('Average sentence len: ', np.mean(sent_len))\n",
        "  print('Max sentence len: ', np.max(sent_len) )\n",
        "  print('{0:0.3f}% of tokens were replaced by unk token index'.format((unk_w/(unk_w+found_word))*100))\n",
        "\n",
        "  return x_matrix.astype(int), y_matrix \n",
        "\n",
        "\n",
        "def create_train_index(tr, w2v_model, max_s_l=128):\n",
        "  w2i = {'pad': 0, 'unk': 1}\n",
        "\n",
        "  #create tokens list and types\n",
        "  tokens = []\n",
        "  for indx, line in tr.iterrows():\n",
        "    tokens.extend(line['text'].split(' '))\n",
        "  types = list(set(tokens))\n",
        "  #create w2i vocab and pretrained emb matrix\n",
        "  emb_matrix = []\n",
        "  emb_matrix.append(np.zeros(300)) #'pad'\n",
        "  emb_matrix.append(np.zeros(300)) #'unk'\n",
        "  s,d = 0,0\n",
        "  for i, t in enumerate(types):\n",
        "    if t in w2v_model:\n",
        "      indx = len(w2i)\n",
        "      w2i[t] = indx\n",
        "      emb_matrix.append(w2v_model[t])\n",
        "      d += 1\n",
        "    else:\n",
        "      s += 1\n",
        "  print('Not found vectors {} out of {} unique words'.format(s, d+s))\n",
        "  i2w = {j:i for i, j in w2i.items()}\n",
        "  #transform train tokens to index in each text\n",
        "  x_tr, y_tr = transform(tr, w2i, len(tr.lbl_index.unique()), max_s_l)\n",
        "  return x_tr, y_tr, w2i, i2w, np.array(emb_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxu-WD37o4Ly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7WcyT2RG9xV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#building the model in keras with embeddings\n",
        "def build_lstm(word2vec_matrix, max_seq_len, num_classes):\n",
        "  model = None\n",
        "  model = Sequential()\n",
        "  model.add(layers.Embedding(word2vec_matrix.shape[0], word2vec_matrix.shape[1], input_length=max_seq_len,  weights=[word2vec_matrix], trainable=False))\n",
        "  model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1 ])\n",
        "  #print(model.summary())\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jV2WkiZUtN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_to_categorical(y):\n",
        "    assert len(y.shape) == 2\n",
        "    return np.argmax(y, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1fOtA2TIMmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_lstm(train_data, test_data, w2v_model, msl, n_c):\n",
        "\n",
        "  #load data\n",
        "  train_x, train_y, word2in, in2word, pretrained_vectors = create_train_index(train_data, w2v_model, msl)\n",
        "  test_x, test_y = transform(test_data, word2in, len(train_data.lbl_index.unique()), train_x.shape[1])\n",
        "\n",
        "  #initialize model\n",
        "  model = build_lstm(pretrained_vectors, msl, n_c)\n",
        "\n",
        "  #implement early stopping\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
        "\n",
        "  #train model\n",
        "  model.fit(train_x, \n",
        "        train_y, \n",
        "        epochs=100000, \n",
        "        callbacks=callbacks,\n",
        "        validation_data=(test_x, test_y), \n",
        "        batch_size=512, \n",
        "        shuffle=True, \n",
        "        verbose=1)\n",
        "  #model.save('checkpoints/lol')\n",
        "  #model = load_model('checkpoints/lol')\n",
        "\n",
        "  #evaluate model\n",
        "  y_pred = model.predict(test_x)\n",
        "  test_y_cat = one_hot_to_categorical(test_y)\n",
        "  y_pred_cat = one_hot_to_categorical(y_pred)\n",
        "  f1 = f1_score(test_y_cat, y_pred_cat, average='micro')\n",
        "  acc = accuracy_score(test_y_cat, y_pred_cat)\n",
        "\n",
        "  print('Best F1:{}, best accuracy:{}'.format(acc, f1))\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDpSbC5UOWze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#building the cnn in keras\n",
        "def build_cnn(word2vec_matrix, max_seq_len, num_classes):\n",
        "  model = None\n",
        "  model = Sequential()\n",
        "  model.add(layers.Embedding(word2vec_matrix.shape[0], word2vec_matrix.shape[1], input_length=max_seq_len,  weights=[word2vec_matrix]))\n",
        "  model.add(layers.Conv1D(256, 5, activation='relu'))\n",
        "  model.add(layers.GlobalMaxPooling1D())\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6aYedlLOeg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_cnn(train_data, test_data, w2v_model, msl, n_c):\n",
        "\n",
        "  #load data\n",
        "  train_x, train_y, word2in, in2word, pretrained_vectors = create_train_index(train_data, w2v_model, msl)\n",
        "  test_x, test_y = transform(test_data, word2in, len(train_data.lbl_index.unique()), train_x.shape[1])\n",
        "\n",
        "  #initialize model\n",
        "  model = build_cnn(pretrained_vectors, msl, n_c)\n",
        "\n",
        "  #implement early stopping\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
        "\n",
        "  #train model\n",
        "  model.fit(train_x, \n",
        "        train_y, \n",
        "        epochs=100000, \n",
        "        callbacks=callbacks,\n",
        "        validation_data=(test_x, test_y), \n",
        "        batch_size=512, \n",
        "        shuffle=True, \n",
        "        verbose=1)\n",
        "  #model.save('checkpoints/lol')\n",
        "  #model = load_model('checkpoints/lol')\n",
        "\n",
        "  #evaluate model\n",
        "  y_pred = model.predict(test_x)\n",
        "  test_y_cat = one_hot_to_categorical(test_y)\n",
        "  y_pred_cat = one_hot_to_categorical(y_pred)\n",
        "  f1 = f1_score(test_y_cat, y_pred_cat, average='micro')\n",
        "  acc = accuracy_score(test_y_cat, y_pred_cat)\n",
        "\n",
        "  print('Best F1:{}, best accuracy:{}'.format(acc, f1))\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AioyBJpijgHG",
        "colab_type": "text"
      },
      "source": [
        "#Trec-50 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKXEpJW77qnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data, l2i = read_trec(6000, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geF5Sa-kIahx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LEN, N_CLASSES = 37, len(train_data.lbl_index.unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hkcILLuJ7gW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a4881b10-2fe2-4a23-d9d4-14b80fbdf951"
      },
      "source": [
        "run_lstm(train_data, test_data, w2v_model, MAX_SEQ_LEN, N_CLASSES )"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not found vectors 262 out of 9448 unique words\n",
            "Average sentence len:  10.204512105649304\n",
            "Max sentence len:  37\n",
            "2.017% of tokens were replaced by unk token index\n",
            "Average sentence len:  7.516\n",
            "Max sentence len:  17\n",
            "9.792% of tokens were replaced by unk token index\n",
            "Train on 5452 samples, validate on 500 samples\n",
            "Epoch 1/100000\n",
            "5452/5452 [==============================] - 5s 840us/step - loss: 3.6479 - accuracy: 0.1282 - f1: 0.0000e+00 - val_loss: 3.2121 - val_accuracy: 0.1100 - val_f1: 0.0000e+00\n",
            "Epoch 2/100000\n",
            "5452/5452 [==============================] - 3s 613us/step - loss: 3.1220 - accuracy: 0.1653 - f1: 0.0000e+00 - val_loss: 3.2496 - val_accuracy: 0.1100 - val_f1: 0.0000e+00\n",
            "Epoch 3/100000\n",
            "5452/5452 [==============================] - 3s 598us/step - loss: 3.0808 - accuracy: 0.1669 - f1: 0.0000e+00 - val_loss: 3.2170 - val_accuracy: 0.1120 - val_f1: 0.0000e+00\n",
            "Epoch 4/100000\n",
            "5452/5452 [==============================] - 3s 584us/step - loss: 3.0653 - accuracy: 0.1708 - f1: 0.0000e+00 - val_loss: 3.2344 - val_accuracy: 0.1100 - val_f1: 0.0000e+00\n",
            "Epoch 5/100000\n",
            "5452/5452 [==============================] - 3s 577us/step - loss: 3.0562 - accuracy: 0.1796 - f1: 0.0000e+00 - val_loss: 3.2024 - val_accuracy: 0.1100 - val_f1: 0.0000e+00\n",
            "Epoch 6/100000\n",
            "5452/5452 [==============================] - 3s 576us/step - loss: 3.0355 - accuracy: 0.1827 - f1: 0.0000e+00 - val_loss: 3.1728 - val_accuracy: 0.1120 - val_f1: 0.0000e+00\n",
            "Epoch 7/100000\n",
            "5452/5452 [==============================] - 3s 606us/step - loss: 2.9821 - accuracy: 0.1847 - f1: 0.0071 - val_loss: 3.0837 - val_accuracy: 0.2320 - val_f1: 0.0000e+00\n",
            "Epoch 8/100000\n",
            "5452/5452 [==============================] - 3s 589us/step - loss: 2.8759 - accuracy: 0.2276 - f1: 0.1308 - val_loss: 2.9704 - val_accuracy: 0.2360 - val_f1: 0.1577\n",
            "Epoch 9/100000\n",
            "5452/5452 [==============================] - 3s 595us/step - loss: 2.7546 - accuracy: 0.2915 - f1: 0.1663 - val_loss: 2.8811 - val_accuracy: 0.2580 - val_f1: 0.1580\n",
            "Epoch 10/100000\n",
            "5452/5452 [==============================] - 3s 602us/step - loss: 2.6339 - accuracy: 0.3120 - f1: 0.1653 - val_loss: 2.7889 - val_accuracy: 0.2540 - val_f1: 0.1580\n",
            "Epoch 11/100000\n",
            "5452/5452 [==============================] - 3s 582us/step - loss: 2.5388 - accuracy: 0.3208 - f1: 0.1605 - val_loss: 2.6851 - val_accuracy: 0.2560 - val_f1: 0.1673\n",
            "Epoch 12/100000\n",
            "5452/5452 [==============================] - 3s 607us/step - loss: 2.4823 - accuracy: 0.3280 - f1: 0.1815 - val_loss: 2.6489 - val_accuracy: 0.2540 - val_f1: 0.1640\n",
            "Epoch 13/100000\n",
            "5452/5452 [==============================] - 3s 591us/step - loss: 2.4384 - accuracy: 0.3397 - f1: 0.1777 - val_loss: 2.5921 - val_accuracy: 0.3980 - val_f1: 0.1610\n",
            "Epoch 14/100000\n",
            "5452/5452 [==============================] - 3s 596us/step - loss: 2.4045 - accuracy: 0.3446 - f1: 0.1982 - val_loss: 2.5595 - val_accuracy: 0.3900 - val_f1: 0.1583\n",
            "Epoch 15/100000\n",
            "5452/5452 [==============================] - 3s 585us/step - loss: 2.3664 - accuracy: 0.3518 - f1: 0.2017 - val_loss: 2.5020 - val_accuracy: 0.4180 - val_f1: 0.1613\n",
            "Epoch 16/100000\n",
            "5452/5452 [==============================] - 3s 595us/step - loss: 2.3229 - accuracy: 0.3716 - f1: 0.2390 - val_loss: 2.4370 - val_accuracy: 0.4540 - val_f1: 0.2292\n",
            "Epoch 17/100000\n",
            "5452/5452 [==============================] - 3s 606us/step - loss: 2.2838 - accuracy: 0.3777 - f1: 0.2507 - val_loss: 2.3535 - val_accuracy: 0.4760 - val_f1: 0.2341\n",
            "Epoch 18/100000\n",
            "5452/5452 [==============================] - 3s 607us/step - loss: 2.2363 - accuracy: 0.3938 - f1: 0.2907 - val_loss: 2.3982 - val_accuracy: 0.4600 - val_f1: 0.3944\n",
            "Epoch 19/100000\n",
            "5452/5452 [==============================] - 3s 601us/step - loss: 2.2575 - accuracy: 0.3910 - f1: 0.3061 - val_loss: 2.3395 - val_accuracy: 0.4660 - val_f1: 0.2617\n",
            "Epoch 20/100000\n",
            "5452/5452 [==============================] - 3s 587us/step - loss: 2.1946 - accuracy: 0.4167 - f1: 0.3163 - val_loss: 2.1987 - val_accuracy: 0.5220 - val_f1: 0.4696\n",
            "Epoch 21/100000\n",
            "5452/5452 [==============================] - 3s 607us/step - loss: 2.1306 - accuracy: 0.4452 - f1: 0.3638 - val_loss: 2.2687 - val_accuracy: 0.4940 - val_f1: 0.5321\n",
            "Epoch 22/100000\n",
            "5452/5452 [==============================] - 3s 599us/step - loss: 2.1032 - accuracy: 0.4596 - f1: 0.3970 - val_loss: 2.2462 - val_accuracy: 0.5020 - val_f1: 0.5718\n",
            "Epoch 23/100000\n",
            "5452/5452 [==============================] - 3s 572us/step - loss: 2.0693 - accuracy: 0.4600 - f1: 0.4099 - val_loss: 2.1029 - val_accuracy: 0.5060 - val_f1: 0.5827\n",
            "Epoch 24/100000\n",
            "5452/5452 [==============================] - 3s 599us/step - loss: 2.0273 - accuracy: 0.4752 - f1: 0.4494 - val_loss: 2.1315 - val_accuracy: 0.5120 - val_f1: 0.5884\n",
            "Epoch 25/100000\n",
            "5452/5452 [==============================] - 3s 595us/step - loss: 2.0166 - accuracy: 0.4747 - f1: 0.4507 - val_loss: 2.1718 - val_accuracy: 0.5060 - val_f1: 0.5733\n",
            "Epoch 26/100000\n",
            "5452/5452 [==============================] - 3s 585us/step - loss: 2.0052 - accuracy: 0.4765 - f1: 0.4666 - val_loss: 2.1145 - val_accuracy: 0.5060 - val_f1: 0.5686\n",
            "Epoch 27/100000\n",
            "5452/5452 [==============================] - 3s 586us/step - loss: 1.9855 - accuracy: 0.4785 - f1: 0.4591 - val_loss: 2.1945 - val_accuracy: 0.4960 - val_f1: 0.5620\n",
            "Epoch 28/100000\n",
            "5452/5452 [==============================] - 3s 587us/step - loss: 1.9679 - accuracy: 0.4771 - f1: 0.4636 - val_loss: 1.9941 - val_accuracy: 0.5500 - val_f1: 0.5859\n",
            "Epoch 29/100000\n",
            "5452/5452 [==============================] - 3s 605us/step - loss: 1.9338 - accuracy: 0.4846 - f1: 0.4691 - val_loss: 2.0208 - val_accuracy: 0.5500 - val_f1: 0.5732\n",
            "Epoch 30/100000\n",
            "5452/5452 [==============================] - 3s 594us/step - loss: 1.8842 - accuracy: 0.5000 - f1: 0.4900 - val_loss: 1.9365 - val_accuracy: 0.5560 - val_f1: 0.5964\n",
            "Epoch 31/100000\n",
            "5452/5452 [==============================] - 3s 595us/step - loss: 1.8637 - accuracy: 0.5064 - f1: 0.5084 - val_loss: 1.9958 - val_accuracy: 0.5120 - val_f1: 0.5956\n",
            "Epoch 32/100000\n",
            "5452/5452 [==============================] - 3s 598us/step - loss: 1.8671 - accuracy: 0.5073 - f1: 0.5170 - val_loss: 1.9275 - val_accuracy: 0.5500 - val_f1: 0.6054\n",
            "Epoch 33/100000\n",
            "5452/5452 [==============================] - 3s 599us/step - loss: 1.8222 - accuracy: 0.5167 - f1: 0.5271 - val_loss: 1.8634 - val_accuracy: 0.5700 - val_f1: 0.6097\n",
            "Epoch 34/100000\n",
            "5452/5452 [==============================] - 3s 594us/step - loss: 1.7800 - accuracy: 0.5240 - f1: 0.5441 - val_loss: 1.8253 - val_accuracy: 0.5860 - val_f1: 0.6339\n",
            "Epoch 35/100000\n",
            "5452/5452 [==============================] - 3s 605us/step - loss: 1.7830 - accuracy: 0.5253 - f1: 0.5406 - val_loss: 1.7882 - val_accuracy: 0.5760 - val_f1: 0.6220\n",
            "Epoch 36/100000\n",
            "5452/5452 [==============================] - 3s 599us/step - loss: 1.7751 - accuracy: 0.5273 - f1: 0.5396 - val_loss: 1.7665 - val_accuracy: 0.5780 - val_f1: 0.6285\n",
            "Epoch 37/100000\n",
            "5452/5452 [==============================] - 3s 577us/step - loss: 1.7307 - accuracy: 0.5360 - f1: 0.5492 - val_loss: 1.7650 - val_accuracy: 0.5780 - val_f1: 0.6359\n",
            "Epoch 38/100000\n",
            "5452/5452 [==============================] - 3s 604us/step - loss: 1.7108 - accuracy: 0.5411 - f1: 0.5542 - val_loss: 1.7464 - val_accuracy: 0.5840 - val_f1: 0.6378\n",
            "Epoch 39/100000\n",
            "5452/5452 [==============================] - 3s 604us/step - loss: 1.7115 - accuracy: 0.5396 - f1: 0.5482 - val_loss: 1.6795 - val_accuracy: 0.5960 - val_f1: 0.6442\n",
            "Epoch 40/100000\n",
            "5452/5452 [==============================] - 3s 598us/step - loss: 1.6821 - accuracy: 0.5504 - f1: 0.5642 - val_loss: 1.6509 - val_accuracy: 0.6060 - val_f1: 0.6486\n",
            "Epoch 41/100000\n",
            "5452/5452 [==============================] - 3s 603us/step - loss: 1.6657 - accuracy: 0.5497 - f1: 0.5578 - val_loss: 1.7555 - val_accuracy: 0.5780 - val_f1: 0.6523\n",
            "Epoch 42/100000\n",
            "5452/5452 [==============================] - 3s 615us/step - loss: 1.6722 - accuracy: 0.5486 - f1: 0.5614 - val_loss: 1.7917 - val_accuracy: 0.5660 - val_f1: 0.6380\n",
            "Epoch 43/100000\n",
            "5452/5452 [==============================] - 3s 578us/step - loss: 1.6501 - accuracy: 0.5536 - f1: 0.5657 - val_loss: 1.8515 - val_accuracy: 0.5900 - val_f1: 0.6079\n",
            "Epoch 44/100000\n",
            "5452/5452 [==============================] - 3s 586us/step - loss: 1.7200 - accuracy: 0.5356 - f1: 0.5514 - val_loss: 1.7843 - val_accuracy: 0.5620 - val_f1: 0.6245\n",
            "Epoch 45/100000\n",
            "5452/5452 [==============================] - 3s 575us/step - loss: 1.6680 - accuracy: 0.5503 - f1: 0.5621 - val_loss: 1.6565 - val_accuracy: 0.6020 - val_f1: 0.6532\n",
            "Best F1:0.602, best accuracy:0.602\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.602"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgW3zSs1Ox9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "87a1caf5-3fa4-4553-961a-f2fd5113fcb0"
      },
      "source": [
        "run_cnn(train_data, test_data, w2v_model, MAX_SEQ_LEN, N_CLASSES)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not found vectors 262 out of 9448 unique words\n",
            "Average sentence len:  10.204512105649304\n",
            "Max sentence len:  37\n",
            "2.017% of tokens were replaced by unk token index\n",
            "Average sentence len:  7.516\n",
            "Max sentence len:  17\n",
            "9.792% of tokens were replaced by unk token index\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5452 samples, validate on 500 samples\n",
            "Epoch 1/100000\n",
            "5452/5452 [==============================] - 0s 84us/step - loss: 3.7233 - accuracy: 0.1434 - f1: 0.0000e+00 - val_loss: 3.4701 - val_accuracy: 0.1100 - val_f1: 0.0000e+00\n",
            "Epoch 2/100000\n",
            "5452/5452 [==============================] - 0s 49us/step - loss: 3.1823 - accuracy: 0.1772 - f1: 0.0000e+00 - val_loss: 3.1358 - val_accuracy: 0.2460 - val_f1: 0.0000e+00\n",
            "Epoch 3/100000\n",
            "5452/5452 [==============================] - 0s 49us/step - loss: 2.9005 - accuracy: 0.2937 - f1: 0.0000e+00 - val_loss: 2.9211 - val_accuracy: 0.4420 - val_f1: 0.0000e+00\n",
            "Epoch 4/100000\n",
            "5452/5452 [==============================] - 0s 49us/step - loss: 2.5817 - accuracy: 0.3298 - f1: 0.0777 - val_loss: 2.5325 - val_accuracy: 0.4440 - val_f1: 0.2292\n",
            "Epoch 5/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 2.1574 - accuracy: 0.4461 - f1: 0.3568 - val_loss: 2.1968 - val_accuracy: 0.4900 - val_f1: 0.4882\n",
            "Epoch 6/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 1.7650 - accuracy: 0.5350 - f1: 0.5195 - val_loss: 1.9712 - val_accuracy: 0.5560 - val_f1: 0.5520\n",
            "Epoch 7/100000\n",
            "5452/5452 [==============================] - 0s 49us/step - loss: 1.4042 - accuracy: 0.6332 - f1: 0.6325 - val_loss: 1.7502 - val_accuracy: 0.5880 - val_f1: 0.6367\n",
            "Epoch 8/100000\n",
            "5452/5452 [==============================] - 0s 49us/step - loss: 1.0661 - accuracy: 0.7447 - f1: 0.7293 - val_loss: 1.5935 - val_accuracy: 0.6200 - val_f1: 0.6777\n",
            "Epoch 9/100000\n",
            "5452/5452 [==============================] - 0s 49us/step - loss: 0.7893 - accuracy: 0.8186 - f1: 0.8107 - val_loss: 1.4735 - val_accuracy: 0.6520 - val_f1: 0.7117\n",
            "Epoch 10/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 0.5742 - accuracy: 0.8766 - f1: 0.8574 - val_loss: 1.3949 - val_accuracy: 0.6880 - val_f1: 0.7286\n",
            "Epoch 11/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 0.4134 - accuracy: 0.9074 - f1: 0.8970 - val_loss: 1.3766 - val_accuracy: 0.7000 - val_f1: 0.7326\n",
            "Epoch 12/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 0.2877 - accuracy: 0.9463 - f1: 0.9316 - val_loss: 1.2994 - val_accuracy: 0.7260 - val_f1: 0.7468\n",
            "Epoch 13/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 0.1994 - accuracy: 0.9664 - f1: 0.9527 - val_loss: 1.3219 - val_accuracy: 0.7120 - val_f1: 0.7554\n",
            "Epoch 14/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 0.1386 - accuracy: 0.9802 - f1: 0.9661 - val_loss: 1.2967 - val_accuracy: 0.7260 - val_f1: 0.7569\n",
            "Epoch 15/100000\n",
            "5452/5452 [==============================] - 0s 49us/step - loss: 0.0960 - accuracy: 0.9897 - f1: 0.9779 - val_loss: 1.3195 - val_accuracy: 0.7400 - val_f1: 0.7621\n",
            "Epoch 16/100000\n",
            "5452/5452 [==============================] - 0s 47us/step - loss: 0.0677 - accuracy: 0.9932 - f1: 0.9855 - val_loss: 1.3378 - val_accuracy: 0.7300 - val_f1: 0.7596\n",
            "Epoch 17/100000\n",
            "5452/5452 [==============================] - 0s 47us/step - loss: 0.0478 - accuracy: 0.9960 - f1: 0.9923 - val_loss: 1.3585 - val_accuracy: 0.7500 - val_f1: 0.7630\n",
            "Epoch 18/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 0.0341 - accuracy: 0.9983 - f1: 0.9954 - val_loss: 1.3693 - val_accuracy: 0.7540 - val_f1: 0.7720\n",
            "Epoch 19/100000\n",
            "5452/5452 [==============================] - 0s 50us/step - loss: 0.0250 - accuracy: 0.9982 - f1: 0.9976 - val_loss: 1.3839 - val_accuracy: 0.7480 - val_f1: 0.7714\n",
            "Best F1:0.748, best accuracy:0.748\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.748"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDNdHFlIVYnO",
        "colab_type": "text"
      },
      "source": [
        "### TREC-50 with augmetaion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZIcM8sNDVxwi",
        "colab": {}
      },
      "source": [
        "train_data, test_data, l2i = read_trec(6000, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nlgfA97WEvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_aug(name, lbl2indx):\n",
        "  aug = pd.read_csv(name, index_col=0)\n",
        "  aug['lbl_index'] = aug.labels.apply(lambda x:lbl2indx[x])\n",
        "  aug = aug[['texts', 'lbl_index']]\n",
        "  aug.columns = ['text', 'lbl_index']\n",
        "  return aug"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t64Bvpv_Vxwn",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LEN, N_CLASSES = 37, len(train_data.lbl_index.unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d7H8pzNXvFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "augmentation = read_aug('trec50_n1_bert_squad_pr_b-0.5_min_b-min_max_b-max_coefs2-2.csv', l2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7tDm5RmZCXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "0eeeda3a-4753-4efa-d93a-d0dfb8f121bd"
      },
      "source": [
        "augmentation"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>lbl_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>After her second solo album, what other entert...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Beyonce's younger sibling also sang with her i...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What town did Beyonce go to school in?</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Who decided to place Beyonce's group in Star S...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"Charlie's Angels\" featured which single from ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89147</th>\n",
              "      <td>Large quantities of what can be created for te...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89148</th>\n",
              "      <td>What did Parmenides say everything was made of?</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89149</th>\n",
              "      <td>How does Descartes use matter and the formal/f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89150</th>\n",
              "      <td>Both primary and secondary properties are suit...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89151</th>\n",
              "      <td>What relation explains the carriers of the ele...</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89152 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  lbl_index\n",
              "0      After her second solo album, what other entert...         14\n",
              "1      Beyonce's younger sibling also sang with her i...          5\n",
              "2                 What town did Beyonce go to school in?         21\n",
              "3      Who decided to place Beyonce's group in Star S...          4\n",
              "4      \"Charlie's Angels\" featured which single from ...          1\n",
              "...                                                  ...        ...\n",
              "89147  Large quantities of what can be created for te...         14\n",
              "89148    What did Parmenides say everything was made of?         28\n",
              "89149  How does Descartes use matter and the formal/f...          0\n",
              "89150  Both primary and secondary properties are suit...         14\n",
              "89151  What relation explains the carriers of the ele...         12\n",
              "\n",
              "[89152 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "07a07c93-c42d-4cfa-aa0e-5b82892f3538",
        "id": "96DcP40kVxwq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "run_lstm(pd.concat([train_data, augmentation]), test_data, w2v_model, MAX_SEQ_LEN, N_CLASSES )"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not found vectors 27947 out of 65944 unique words\n",
            "Average sentence len:  10.32127605598072\n",
            "Max sentence len:  25601\n",
            "12.269% of tokens were replaced by unk token index\n",
            "Average sentence len:  7.516\n",
            "Max sentence len:  17\n",
            "4.604% of tokens were replaced by unk token index\n",
            "Train on 94604 samples, validate on 500 samples\n",
            "Epoch 1/100000\n",
            "94604/94604 [==============================] - 56s 590us/step - loss: 2.4234 - accuracy: 0.3710 - f1: 0.1354 - val_loss: 3.1244 - val_accuracy: 0.2700 - val_f1: 0.1812\n",
            "Epoch 2/100000\n",
            "94604/94604 [==============================] - 55s 584us/step - loss: 1.8258 - accuracy: 0.5342 - f1: 0.4015 - val_loss: 2.5717 - val_accuracy: 0.3500 - val_f1: 0.2968\n",
            "Epoch 3/100000\n",
            "94604/94604 [==============================] - 55s 583us/step - loss: 1.6626 - accuracy: 0.5683 - f1: 0.4763 - val_loss: 2.3491 - val_accuracy: 0.3640 - val_f1: 0.3571\n",
            "Epoch 4/100000\n",
            "94604/94604 [==============================] - 55s 582us/step - loss: 1.5664 - accuracy: 0.5811 - f1: 0.5244 - val_loss: 2.3780 - val_accuracy: 0.3480 - val_f1: 0.3723\n",
            "Epoch 5/100000\n",
            "94604/94604 [==============================] - 57s 600us/step - loss: 1.4796 - accuracy: 0.5975 - f1: 0.5581 - val_loss: 2.1001 - val_accuracy: 0.5580 - val_f1: 0.3767\n",
            "Epoch 6/100000\n",
            "94604/94604 [==============================] - 58s 611us/step - loss: 1.3881 - accuracy: 0.6168 - f1: 0.5894 - val_loss: 2.0158 - val_accuracy: 0.5980 - val_f1: 0.5562\n",
            "Epoch 7/100000\n",
            "94604/94604 [==============================] - 57s 604us/step - loss: 1.3135 - accuracy: 0.6371 - f1: 0.6144 - val_loss: 1.8106 - val_accuracy: 0.6160 - val_f1: 0.6194\n",
            "Epoch 8/100000\n",
            "94604/94604 [==============================] - 56s 597us/step - loss: 1.2482 - accuracy: 0.6515 - f1: 0.6317 - val_loss: 1.7131 - val_accuracy: 0.6240 - val_f1: 0.6382\n",
            "Epoch 9/100000\n",
            "94604/94604 [==============================] - 55s 579us/step - loss: 1.1751 - accuracy: 0.6725 - f1: 0.6554 - val_loss: 1.5951 - val_accuracy: 0.6260 - val_f1: 0.6479\n",
            "Epoch 10/100000\n",
            "94604/94604 [==============================] - 53s 564us/step - loss: 1.1108 - accuracy: 0.6927 - f1: 0.6806 - val_loss: 1.5754 - val_accuracy: 0.6520 - val_f1: 0.6716\n",
            "Epoch 11/100000\n",
            "94604/94604 [==============================] - 53s 556us/step - loss: 1.0525 - accuracy: 0.7091 - f1: 0.7002 - val_loss: 1.4481 - val_accuracy: 0.6660 - val_f1: 0.6796\n",
            "Epoch 12/100000\n",
            "94604/94604 [==============================] - 53s 559us/step - loss: 0.9997 - accuracy: 0.7228 - f1: 0.7172 - val_loss: 1.3989 - val_accuracy: 0.6880 - val_f1: 0.7198\n",
            "Epoch 13/100000\n",
            "94604/94604 [==============================] - 53s 560us/step - loss: 0.9553 - accuracy: 0.7367 - f1: 0.7314 - val_loss: 1.3600 - val_accuracy: 0.7060 - val_f1: 0.7177\n",
            "Epoch 14/100000\n",
            "94604/94604 [==============================] - 52s 553us/step - loss: 0.9157 - accuracy: 0.7463 - f1: 0.7426 - val_loss: 1.2651 - val_accuracy: 0.7060 - val_f1: 0.7294\n",
            "Epoch 15/100000\n",
            "94604/94604 [==============================] - 52s 551us/step - loss: 0.8744 - accuracy: 0.7570 - f1: 0.7555 - val_loss: 1.2772 - val_accuracy: 0.7200 - val_f1: 0.7385\n",
            "Epoch 16/100000\n",
            "94604/94604 [==============================] - 52s 549us/step - loss: 0.8409 - accuracy: 0.7657 - f1: 0.7648 - val_loss: 1.1597 - val_accuracy: 0.7400 - val_f1: 0.7425\n",
            "Epoch 17/100000\n",
            "94604/94604 [==============================] - 52s 554us/step - loss: 0.8062 - accuracy: 0.7739 - f1: 0.7734 - val_loss: 1.2367 - val_accuracy: 0.7260 - val_f1: 0.7442\n",
            "Epoch 18/100000\n",
            "94604/94604 [==============================] - 52s 549us/step - loss: 0.7760 - accuracy: 0.7811 - f1: 0.7814 - val_loss: 1.1044 - val_accuracy: 0.7440 - val_f1: 0.7647\n",
            "Epoch 19/100000\n",
            "94604/94604 [==============================] - 52s 552us/step - loss: 0.7505 - accuracy: 0.7888 - f1: 0.7892 - val_loss: 1.0440 - val_accuracy: 0.7680 - val_f1: 0.7760\n",
            "Epoch 20/100000\n",
            "94604/94604 [==============================] - 52s 549us/step - loss: 0.7239 - accuracy: 0.7953 - f1: 0.7965 - val_loss: 1.1046 - val_accuracy: 0.7560 - val_f1: 0.7768\n",
            "Epoch 21/100000\n",
            "94604/94604 [==============================] - 52s 551us/step - loss: 0.6970 - accuracy: 0.8014 - f1: 0.8030 - val_loss: 1.0085 - val_accuracy: 0.7600 - val_f1: 0.7880\n",
            "Epoch 22/100000\n",
            "94604/94604 [==============================] - 52s 546us/step - loss: 0.6719 - accuracy: 0.8086 - f1: 0.8101 - val_loss: 0.9845 - val_accuracy: 0.7780 - val_f1: 0.7935\n",
            "Epoch 23/100000\n",
            "94604/94604 [==============================] - 52s 549us/step - loss: 0.6478 - accuracy: 0.8146 - f1: 0.8161 - val_loss: 0.9482 - val_accuracy: 0.7820 - val_f1: 0.8022\n",
            "Epoch 24/100000\n",
            "94604/94604 [==============================] - 52s 546us/step - loss: 0.6289 - accuracy: 0.8203 - f1: 0.8218 - val_loss: 0.9715 - val_accuracy: 0.7800 - val_f1: 0.8004\n",
            "Epoch 25/100000\n",
            "94604/94604 [==============================] - 51s 544us/step - loss: 0.6172 - accuracy: 0.8226 - f1: 0.8246 - val_loss: 0.9065 - val_accuracy: 0.8020 - val_f1: 0.8177\n",
            "Epoch 26/100000\n",
            "94604/94604 [==============================] - 52s 548us/step - loss: 0.5882 - accuracy: 0.8289 - f1: 0.8326 - val_loss: 0.8927 - val_accuracy: 0.7980 - val_f1: 0.8173\n",
            "Epoch 27/100000\n",
            "94604/94604 [==============================] - 52s 546us/step - loss: 0.5649 - accuracy: 0.8354 - f1: 0.8379 - val_loss: 0.8209 - val_accuracy: 0.8060 - val_f1: 0.8263\n",
            "Epoch 28/100000\n",
            "94604/94604 [==============================] - 52s 547us/step - loss: 0.5477 - accuracy: 0.8395 - f1: 0.8428 - val_loss: 0.8369 - val_accuracy: 0.7900 - val_f1: 0.8138\n",
            "Epoch 29/100000\n",
            "94604/94604 [==============================] - 52s 546us/step - loss: 0.5326 - accuracy: 0.8438 - f1: 0.8463 - val_loss: 0.8148 - val_accuracy: 0.8140 - val_f1: 0.8301\n",
            "Epoch 30/100000\n",
            "94604/94604 [==============================] - 51s 542us/step - loss: 0.5145 - accuracy: 0.8486 - f1: 0.8522 - val_loss: 0.8417 - val_accuracy: 0.8200 - val_f1: 0.8353\n",
            "Epoch 31/100000\n",
            "94604/94604 [==============================] - 51s 543us/step - loss: 0.4993 - accuracy: 0.8539 - f1: 0.8565 - val_loss: 0.8213 - val_accuracy: 0.8260 - val_f1: 0.8386\n",
            "Epoch 32/100000\n",
            "94604/94604 [==============================] - 52s 548us/step - loss: 0.4799 - accuracy: 0.8579 - f1: 0.8621 - val_loss: 0.8628 - val_accuracy: 0.8040 - val_f1: 0.8267\n",
            "Epoch 33/100000\n",
            "94604/94604 [==============================] - 52s 545us/step - loss: 0.4606 - accuracy: 0.8631 - f1: 0.8665 - val_loss: 0.8877 - val_accuracy: 0.8120 - val_f1: 0.8227\n",
            "Epoch 34/100000\n",
            "94604/94604 [==============================] - 52s 547us/step - loss: 0.4491 - accuracy: 0.8671 - f1: 0.8698 - val_loss: 0.8672 - val_accuracy: 0.8240 - val_f1: 0.8372\n",
            "Best F1:0.824, best accuracy:0.824\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.824"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3373a0ed-30ff-4cae-88b1-aa7b1e81e77a",
        "id": "-WUzXbxFVxwv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "run_cnn(pd.concat([train_data, augmentation]), test_data, w2v_model, MAX_SEQ_LEN, N_CLASSES)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not found vectors 27947 out of 65944 unique words\n",
            "Average sentence len:  10.32127605598072\n",
            "Max sentence len:  25601\n",
            "12.269% of tokens were replaced by unk token index\n",
            "Average sentence len:  7.516\n",
            "Max sentence len:  17\n",
            "4.604% of tokens were replaced by unk token index\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 94604 samples, validate on 500 samples\n",
            "Epoch 1/100000\n",
            "94604/94604 [==============================] - 6s 66us/step - loss: 1.5710 - accuracy: 0.6126 - f1: 0.5381 - val_loss: 1.3110 - val_accuracy: 0.7560 - val_f1: 0.7776\n",
            "Epoch 2/100000\n",
            "94604/94604 [==============================] - 6s 64us/step - loss: 0.5106 - accuracy: 0.8664 - f1: 0.8684 - val_loss: 0.9257 - val_accuracy: 0.8100 - val_f1: 0.8364\n",
            "Epoch 3/100000\n",
            "94604/94604 [==============================] - 6s 64us/step - loss: 0.2475 - accuracy: 0.9341 - f1: 0.9357 - val_loss: 0.8187 - val_accuracy: 0.8300 - val_f1: 0.8443\n",
            "Epoch 4/100000\n",
            "94604/94604 [==============================] - 6s 64us/step - loss: 0.1243 - accuracy: 0.9676 - f1: 0.9683 - val_loss: 0.8806 - val_accuracy: 0.8380 - val_f1: 0.8453\n",
            "Epoch 5/100000\n",
            "94604/94604 [==============================] - 6s 64us/step - loss: 0.0679 - accuracy: 0.9836 - f1: 0.9839 - val_loss: 0.9141 - val_accuracy: 0.8540 - val_f1: 0.8598\n",
            "Epoch 6/100000\n",
            "94604/94604 [==============================] - 6s 64us/step - loss: 0.0409 - accuracy: 0.9906 - f1: 0.9908 - val_loss: 0.9768 - val_accuracy: 0.8440 - val_f1: 0.8504\n",
            "Epoch 7/100000\n",
            "94604/94604 [==============================] - 6s 64us/step - loss: 0.0271 - accuracy: 0.9939 - f1: 0.9938 - val_loss: 1.0134 - val_accuracy: 0.8540 - val_f1: 0.8586\n",
            "Epoch 8/100000\n",
            "94604/94604 [==============================] - 6s 64us/step - loss: 0.0211 - accuracy: 0.9950 - f1: 0.9952 - val_loss: 1.0939 - val_accuracy: 0.8480 - val_f1: 0.8510\n",
            "Best F1:0.848, best accuracy:0.848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.848"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mKpmkYUlD7U",
        "colab_type": "text"
      },
      "source": [
        "#Trec-6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "De6pEECYPhTF",
        "colab": {}
      },
      "source": [
        "train_data, test_data, l2i = read_trec(6000, typ=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K627ofeFPhTJ",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LEN, N_CLASSES = 37, len(train_data.lbl_index.unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a90aaa52-b479-4644-ae29-27011cefa117",
        "id": "fH078t30PhTN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        }
      },
      "source": [
        "run_lstm(train_data, test_data, w2v_model, MAX_SEQ_LEN, N_CLASSES)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not found vectors 262 out of 9448 unique words\n",
            "Average sentence len:  10.204512105649304\n",
            "Max sentence len:  37\n",
            "2.017% of tokens were replaced by unk token index\n",
            "Average sentence len:  7.516\n",
            "Max sentence len:  17\n",
            "9.792% of tokens were replaced by unk token index\n",
            "Train on 5452 samples, validate on 500 samples\n",
            "Epoch 1/100000\n",
            "5452/5452 [==============================] - 4s 807us/step - loss: 1.7272 - accuracy: 0.2273 - f1: 0.0000e+00 - val_loss: 1.6620 - val_accuracy: 0.2960 - val_f1: 0.0000e+00\n",
            "Epoch 2/100000\n",
            "5120/5452 [===========================>..] - ETA: 0s - loss: 1.6487 - accuracy: 0.2510 - f1: 0.0000e+00"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-150-a01d15271fc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-139-32d9007a5a48>\u001b[0m in \u001b[0;36mrun_lstm\u001b[0;34m(train_data, test_data, w2v_model, msl, n_c)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         verbose=1)\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0;31m#model.save('checkpoints/lol')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m#model = load_model('checkpoints/lol')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-TQjBdXlSPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "e570d2e5-9263-418d-a421-88e93fac0da8"
      },
      "source": [
        "run_cnn(train_data, test_data, w2v_model,MAX_SEQ_LEN, N_CLASSES)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not found vectors 262 out of 9448 unique words\n",
            "Average sentence len:  10.204512105649304\n",
            "Max sentence len:  37\n",
            "2.017% of tokens were replaced by unk token index\n",
            "Average sentence len:  7.516\n",
            "Max sentence len:  17\n",
            "9.792% of tokens were replaced by unk token index\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5452 samples, validate on 500 samples\n",
            "Epoch 1/100000\n",
            "5452/5452 [==============================] - 0s 86us/step - loss: 1.7058 - accuracy: 0.2735 - f1: 0.0000e+00 - val_loss: 1.6018 - val_accuracy: 0.4400 - val_f1: 0.0000e+00\n",
            "Epoch 2/100000\n",
            "5452/5452 [==============================] - 0s 49us/step - loss: 1.5215 - accuracy: 0.4182 - f1: 0.0080 - val_loss: 1.3221 - val_accuracy: 0.4780 - val_f1: 0.3121\n",
            "Epoch 3/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 1.2099 - accuracy: 0.6016 - f1: 0.2855 - val_loss: 0.9695 - val_accuracy: 0.6500 - val_f1: 0.5501\n",
            "Epoch 4/100000\n",
            "5452/5452 [==============================] - 0s 47us/step - loss: 0.7607 - accuracy: 0.7727 - f1: 0.6672 - val_loss: 0.6365 - val_accuracy: 0.7940 - val_f1: 0.7523\n",
            "Epoch 5/100000\n",
            "5452/5452 [==============================] - 0s 47us/step - loss: 0.3716 - accuracy: 0.9116 - f1: 0.8998 - val_loss: 0.3953 - val_accuracy: 0.8620 - val_f1: 0.8640\n",
            "Epoch 6/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 0.1510 - accuracy: 0.9648 - f1: 0.9628 - val_loss: 0.3414 - val_accuracy: 0.8960 - val_f1: 0.8941\n",
            "Epoch 7/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 0.0633 - accuracy: 0.9851 - f1: 0.9851 - val_loss: 0.3226 - val_accuracy: 0.8980 - val_f1: 0.9034\n",
            "Epoch 8/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 0.0266 - accuracy: 0.9952 - f1: 0.9956 - val_loss: 0.2995 - val_accuracy: 0.9040 - val_f1: 0.9052\n",
            "Epoch 9/100000\n",
            "5452/5452 [==============================] - 0s 47us/step - loss: 0.0125 - accuracy: 0.9982 - f1: 0.9980 - val_loss: 0.3160 - val_accuracy: 0.9000 - val_f1: 0.9052\n",
            "Epoch 10/100000\n",
            "5452/5452 [==============================] - 0s 47us/step - loss: 0.0073 - accuracy: 0.9993 - f1: 0.9992 - val_loss: 0.3318 - val_accuracy: 0.9020 - val_f1: 0.9032\n",
            "Epoch 11/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 0.0047 - accuracy: 0.9994 - f1: 0.9995 - val_loss: 0.3298 - val_accuracy: 0.9000 - val_f1: 0.9054\n",
            "Epoch 12/100000\n",
            "5452/5452 [==============================] - 0s 47us/step - loss: 0.0033 - accuracy: 0.9998 - f1: 0.9998 - val_loss: 0.3248 - val_accuracy: 0.9000 - val_f1: 0.9052\n",
            "Epoch 13/100000\n",
            "5452/5452 [==============================] - 0s 48us/step - loss: 0.0026 - accuracy: 0.9998 - f1: 0.9998 - val_loss: 0.3346 - val_accuracy: 0.9000 - val_f1: 0.9054\n",
            "Best F1:0.9, best accuracy:0.9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn0nF29z2iiw",
        "colab_type": "text"
      },
      "source": [
        "#SST-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAGFd8Ll0IB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SST2 https://www.kaggle.com/atulanandjha/distillbert-extensive-tutorial-starter-kernel/data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km0S7Mnvxpdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "def read_sst2(n_examples, use_dev=False):\n",
        "  train = pd.read_csv('SST/train.tsv', sep='\\t')[:n_examples]\n",
        "  if use_dev:\n",
        "    dev = pd.read_csv('SST/dev.tsv', sep='\\t')\n",
        "    train = pd.concat([train, dev]).reset_index(drop=True)[:n_examples]\n",
        "    print(train.head())\n",
        "    train.columns = ['text', 'lbl_index']\n",
        "  train.columns = ['text', 'lbl_index']\n",
        "  test = pd.read_csv('SST/test.tsv', sep='\\t')\n",
        "  test.columns = ['text', 'lbl_index']\n",
        "  return train, test, {'neg': 0, 'pos': 1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-iijG6_0drn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bc760a29-c364-445b-9527-54bac2580919"
      },
      "source": [
        "train_sst, test_sst, lbl2ind_sst = read_sst2(10000, True)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             Reviews  Ratings\n",
            "0  a stirring , funny and finally transporting re...        1\n",
            "1  apparently reassembled from the cutting room f...        0\n",
            "2  they presume their audience wo n't sit still f...        0\n",
            "3  this is a visually stunning rumination on love...        1\n",
            "4  jonathan parker 's bartleby should have been t...        1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbM8sK9Y0g6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "f8654db7-702a-4fb9-c766-28a5f0dc9697"
      },
      "source": [
        "run_cnn(train_sst, test_sst, w2v_model, 53, 2)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not found vectors 809 out of 14648 unique words\n",
            "Average sentence len:  18.535292607802873\n",
            "Max sentence len:  51\n",
            "2.344% of tokens were replaced by unk token index\n",
            "Average sentence len:  18.480505216913784\n",
            "Max sentence len:  53\n",
            "6.677% of tokens were replaced by unk token index\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 7792 samples, validate on 1821 samples\n",
            "Epoch 1/100000\n",
            "7792/7792 [==============================] - 1s 140us/step - loss: 0.6841 - accuracy: 0.5675 - f1: 0.5703 - val_loss: 0.6638 - val_accuracy: 0.6107 - val_f1: 0.6099\n",
            "Epoch 2/100000\n",
            "7792/7792 [==============================] - 1s 66us/step - loss: 0.5678 - accuracy: 0.7465 - f1: 0.7489 - val_loss: 0.4691 - val_accuracy: 0.7787 - val_f1: 0.7772\n",
            "Epoch 3/100000\n",
            "7792/7792 [==============================] - 1s 66us/step - loss: 0.2824 - accuracy: 0.8928 - f1: 0.8941 - val_loss: 0.4311 - val_accuracy: 0.8193 - val_f1: 0.8215\n",
            "Epoch 4/100000\n",
            "7792/7792 [==============================] - 1s 65us/step - loss: 0.1160 - accuracy: 0.9607 - f1: 0.9613 - val_loss: 0.4924 - val_accuracy: 0.8155 - val_f1: 0.8165\n",
            "Epoch 5/100000\n",
            "7792/7792 [==============================] - 1s 65us/step - loss: 0.0434 - accuracy: 0.9906 - f1: 0.9911 - val_loss: 0.5902 - val_accuracy: 0.8045 - val_f1: 0.8052\n",
            "Epoch 6/100000\n",
            "7792/7792 [==============================] - 1s 65us/step - loss: 0.0174 - accuracy: 0.9977 - f1: 0.9974 - val_loss: 0.6865 - val_accuracy: 0.8012 - val_f1: 0.8015\n",
            "Epoch 7/100000\n",
            "7792/7792 [==============================] - 1s 65us/step - loss: 0.0087 - accuracy: 0.9988 - f1: 0.9985 - val_loss: 0.7603 - val_accuracy: 0.8045 - val_f1: 0.8040\n",
            "Epoch 8/100000\n",
            "7792/7792 [==============================] - 1s 65us/step - loss: 0.0053 - accuracy: 0.9995 - f1: 0.9991 - val_loss: 0.8282 - val_accuracy: 0.7990 - val_f1: 0.7999\n",
            "Best F1:0.7990115321252059, best accuracy:0.7990115321252059\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7990115321252059"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDtY4CzG1z7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "outputId": "766e28a3-b239-4c32-9ee1-04a2833dd657"
      },
      "source": [
        "run_lstm(train_sst, test_sst, w2v_model, 53, 2)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not found vectors 809 out of 14648 unique words\n",
            "Average sentence len:  18.535292607802873\n",
            "Max sentence len:  51\n",
            "2.344% of tokens were replaced by unk token index\n",
            "Average sentence len:  18.480505216913784\n",
            "Max sentence len:  53\n",
            "6.677% of tokens were replaced by unk token index\n",
            "Train on 7792 samples, validate on 1821 samples\n",
            "Epoch 1/100000\n",
            "7792/7792 [==============================] - 8s 988us/step - loss: 0.6882 - accuracy: 0.5424 - f1: 0.5412 - val_loss: 0.6760 - val_accuracy: 0.5854 - val_f1: 0.5890\n",
            "Epoch 2/100000\n",
            "7792/7792 [==============================] - 6s 808us/step - loss: 0.6585 - accuracy: 0.6127 - f1: 0.6155 - val_loss: 0.6063 - val_accuracy: 0.6798 - val_f1: 0.6803\n",
            "Epoch 3/100000\n",
            "7792/7792 [==============================] - 6s 804us/step - loss: 0.5896 - accuracy: 0.6902 - f1: 0.6905 - val_loss: 0.5996 - val_accuracy: 0.6749 - val_f1: 0.6752\n",
            "Epoch 4/100000\n",
            "7792/7792 [==============================] - 6s 812us/step - loss: 0.5473 - accuracy: 0.7284 - f1: 0.7286 - val_loss: 0.5312 - val_accuracy: 0.7403 - val_f1: 0.7438\n",
            "Epoch 5/100000\n",
            "7792/7792 [==============================] - 6s 810us/step - loss: 0.5170 - accuracy: 0.7492 - f1: 0.7471 - val_loss: 0.5066 - val_accuracy: 0.7589 - val_f1: 0.7627\n",
            "Epoch 6/100000\n",
            "7792/7792 [==============================] - 6s 799us/step - loss: 0.4988 - accuracy: 0.7608 - f1: 0.7620 - val_loss: 0.5070 - val_accuracy: 0.7507 - val_f1: 0.7523\n",
            "Epoch 7/100000\n",
            "7792/7792 [==============================] - 6s 805us/step - loss: 0.4764 - accuracy: 0.7730 - f1: 0.7745 - val_loss: 0.4776 - val_accuracy: 0.7573 - val_f1: 0.7601\n",
            "Epoch 8/100000\n",
            "7792/7792 [==============================] - 6s 792us/step - loss: 0.4669 - accuracy: 0.7791 - f1: 0.7803 - val_loss: 0.5183 - val_accuracy: 0.7364 - val_f1: 0.7376\n",
            "Epoch 9/100000\n",
            "7792/7792 [==============================] - 6s 808us/step - loss: 0.4740 - accuracy: 0.7731 - f1: 0.7720 - val_loss: 0.5401 - val_accuracy: 0.7337 - val_f1: 0.7352\n",
            "Epoch 10/100000\n",
            "7792/7792 [==============================] - 6s 809us/step - loss: 0.4740 - accuracy: 0.7746 - f1: 0.7774 - val_loss: 0.6027 - val_accuracy: 0.7177 - val_f1: 0.7191\n",
            "Epoch 11/100000\n",
            "7792/7792 [==============================] - 6s 810us/step - loss: 0.4679 - accuracy: 0.7861 - f1: 0.7865 - val_loss: 0.4742 - val_accuracy: 0.7683 - val_f1: 0.7694\n",
            "Epoch 12/100000\n",
            "7792/7792 [==============================] - 6s 829us/step - loss: 0.4493 - accuracy: 0.7940 - f1: 0.7932 - val_loss: 0.4736 - val_accuracy: 0.7716 - val_f1: 0.7728\n",
            "Epoch 13/100000\n",
            "7792/7792 [==============================] - 6s 807us/step - loss: 0.4433 - accuracy: 0.7956 - f1: 0.7938 - val_loss: 0.4620 - val_accuracy: 0.7688 - val_f1: 0.7711\n",
            "Epoch 14/100000\n",
            "7792/7792 [==============================] - 6s 800us/step - loss: 0.4399 - accuracy: 0.7958 - f1: 0.7940 - val_loss: 0.5021 - val_accuracy: 0.7787 - val_f1: 0.7795\n",
            "Epoch 15/100000\n",
            "7792/7792 [==============================] - 6s 800us/step - loss: 0.4402 - accuracy: 0.7963 - f1: 0.7984 - val_loss: 0.4871 - val_accuracy: 0.7705 - val_f1: 0.7733\n",
            "Epoch 16/100000\n",
            "7792/7792 [==============================] - 6s 819us/step - loss: 0.4297 - accuracy: 0.7970 - f1: 0.7986 - val_loss: 0.4740 - val_accuracy: 0.7666 - val_f1: 0.7688\n",
            "Epoch 17/100000\n",
            "7792/7792 [==============================] - 6s 823us/step - loss: 0.4292 - accuracy: 0.7974 - f1: 0.7968 - val_loss: 0.4658 - val_accuracy: 0.7770 - val_f1: 0.7784\n",
            "Epoch 18/100000\n",
            "7792/7792 [==============================] - 6s 794us/step - loss: 0.4179 - accuracy: 0.8038 - f1: 0.8029 - val_loss: 0.4619 - val_accuracy: 0.7748 - val_f1: 0.7772\n",
            "Epoch 19/100000\n",
            "7792/7792 [==============================] - 6s 807us/step - loss: 0.4069 - accuracy: 0.8134 - f1: 0.8142 - val_loss: 0.4838 - val_accuracy: 0.7776 - val_f1: 0.7789\n",
            "Epoch 20/100000\n",
            "7792/7792 [==============================] - 6s 802us/step - loss: 0.3979 - accuracy: 0.8149 - f1: 0.8131 - val_loss: 0.4723 - val_accuracy: 0.7738 - val_f1: 0.7751\n",
            "Epoch 21/100000\n",
            "7792/7792 [==============================] - 6s 800us/step - loss: 0.4062 - accuracy: 0.8099 - f1: 0.8083 - val_loss: 0.4656 - val_accuracy: 0.7803 - val_f1: 0.7825\n",
            "Epoch 22/100000\n",
            "7792/7792 [==============================] - 6s 810us/step - loss: 0.3949 - accuracy: 0.8187 - f1: 0.8184 - val_loss: 0.4640 - val_accuracy: 0.7864 - val_f1: 0.7863\n",
            "Epoch 23/100000\n",
            "7792/7792 [==============================] - 6s 816us/step - loss: 0.3895 - accuracy: 0.8208 - f1: 0.8191 - val_loss: 0.5040 - val_accuracy: 0.7825 - val_f1: 0.7821\n",
            "Best F1:0.7825370675453048, best accuracy:0.7825370675453048\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7825370675453048"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irP6y2uW4Tnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BIBMocsVRQz",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yljk7RSXO6Mb",
        "colab_type": "text"
      },
      "source": [
        "OLD EDA code without batching embeding vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_k9j8niBZIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#getting the x and y inputs in numpy array form from the text file\n",
        "def get_x_y(train_data, num_classes, word2vec, word2vec_len=300, input_size=64):\n",
        "  num_lines = train_data.shape[0]\n",
        "\n",
        "  #initialize x and y matrix\n",
        "  x_matrix = None\n",
        "  y_matrix = None\n",
        "\n",
        "  try:\n",
        "    x_matrix = np.zeros((num_lines, input_size, word2vec_len))\n",
        "  except:\n",
        "    print(\"Error!\", num_lines, input_size, word2vec_len)\n",
        "  y_matrix = np.zeros((num_lines, num_classes))\n",
        "  s,d = 0,0\n",
        "  #insert values\n",
        "  for i, line in enumerate(train_data.iterrows()):\n",
        "    label = line[1]['lbl_index']\n",
        "    sentence = line[1]['texts']\n",
        "\n",
        "    #insert x\n",
        "    words = sentence.split(' ')\n",
        "    words = words[:x_matrix.shape[1]] #cut off if too long\n",
        "    for j, word in enumerate(words):\n",
        "      if word in word2vec:\n",
        "        x_matrix[i, j, :] = word2vec[word]\n",
        "        d += 1\n",
        "      else:\n",
        "        s += 1\n",
        "\n",
        "    #insert y\n",
        "    y_matrix[i][label] = 1.0\n",
        "  print('Not found vectors {} out of {} words in dateset'.format(s, d))\n",
        "  return x_matrix, y_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf7krrcZBnKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################################################\n",
        "##################### model #######################\n",
        "###################################################\n",
        "\n",
        "#building the model in keras\n",
        "def build_model(sentence_length, word2vec_len, num_classes):\n",
        "\tmodel = None\n",
        "\tmodel = Sequential()\n",
        " \n",
        "\tmodel.add(Bidirectional(LSTM(256, return_sequences=True), input_shape=(sentence_length, word2vec_len)))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Dense(100, activation='relu'))\n",
        "\tmodel.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1 ])\n",
        "\t#print(model.summary())\n",
        "\treturn model\n",
        "\n",
        "#building the cnn in keras\n",
        "def build_cnn(sentence_length, word2vec_matrix, max_seq_len, num_classes):\n",
        "\tmodel = None\n",
        "\tmodel = Sequential()\n",
        "  model.add(Embedding(word2vec_matrix.shape[0], word2vec_matrix[1], input_length=max_seq_len,  weights=[word2vec_matrix]))\n",
        "\tmodel.add(layers.Conv1D(256, 5, activation='relu', input_shape=(sentence_length, word2vec_len)))\n",
        "\tmodel.add(layers.GlobalMaxPooling1D())\n",
        "\tmodel.add(Dense(100, activation='relu'))\n",
        "\tmodel.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1])\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vWShG8FBjhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(train_data, test_data, num_classes, word2vec, input_size=64, word2vec_len=300):\n",
        "\n",
        "  #initialize model\n",
        "  model = build_model(input_size, word2vec_len, num_classes)\n",
        "\n",
        "  #load data\n",
        "  train_x, train_y = get_x_y(train_data, num_classes, w2v_model)\n",
        "  test_x, test_y = get_x_y(test_data, num_classes, w2v_model)\n",
        "\n",
        "  #implement early stopping\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
        "\n",
        "  #train model\n",
        "  model.fit(train_x, \n",
        "        train_y, \n",
        "        epochs=100000, \n",
        "        callbacks=callbacks,\n",
        "        validation_data=(test_x, test_y), \n",
        "        batch_size=512, \n",
        "        shuffle=True, \n",
        "        verbose=1)\n",
        "  #model.save('checkpoints/lol')\n",
        "  #model = load_model('checkpoints/lol')\n",
        "\n",
        "  #evaluate model\n",
        "  y_pred = model.predict(test_x)\n",
        "  test_y_cat = one_hot_to_categorical(test_y)\n",
        "  y_pred_cat = one_hot_to_categorical(y_pred)\n",
        "  f1 = f1_score(test_y_cat, y_pred_cat, average='micro')\n",
        "  acc = accuracy_score(test_y_cat, y_pred_cat)\n",
        "\n",
        "  print('Best F1:{}, best accuracy:{}'.format(acc, f1))\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIrDe2m7Vviu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_cnn(train_data, test_data, num_classes, word2vec, input_size =64, word2vec_len=300):\n",
        "\n",
        "  #initialize model\n",
        "  model = build_cnn(input_size, word2vec_len, num_classes)\n",
        "\n",
        "  #load data\n",
        "  train_x, train_y = get_x_y(train_data, num_classes, w2v_model)\n",
        "  test_x, test_y = get_x_y(test_data, num_classes, w2v_model)\n",
        "\n",
        "  #implement early stopping\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
        "\n",
        "  #train model\n",
        "  model.fit(\ttrain_x, \n",
        "        train_y, \n",
        "        epochs=100000, \n",
        "        callbacks=callbacks,\n",
        "        validation_data=(test_x, test_y), \n",
        "        batch_size=1024, \n",
        "        shuffle=True, \n",
        "        verbose=1)\n",
        "  #model.save('checkpoints/lol')\n",
        "  #model = load_model('checkpoints/lol')\n",
        "\n",
        "  #evaluate model\n",
        "  \n",
        "  y_pred = model.predict(test_x)\n",
        "  test_y_cat = one_hot_to_categorical(test_y)\n",
        "  y_pred_cat = one_hot_to_categorical(y_pred)\n",
        "  f1 = f1_score(test_y_cat, y_pred_cat, average='micro')\n",
        "  acc = accuracy_score(test_y_cat, y_pred_cat)\n",
        "  \n",
        "  print('Best F1:{}, best accuracy:{}'.format(acc, f1))\n",
        "\n",
        "  #clean memory???\n",
        "  train_x, train_y, model = None, None, None\n",
        "  gc.collect()\n",
        "\n",
        "  #return the accuracy\n",
        "  #print(\"data with shape:\", train_x.shape, train_y.shape, 'train=', train_file, 'test=', test_file, 'with fraction', percent_dataset, 'had acc', acc)\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}