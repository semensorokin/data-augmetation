{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataAugmentationWithBert.ipynb","provenance":[],"authorship_tag":"ABX9TyOh95X9Fd8CAsgc/BvE3qf3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ohgjAMSfZ8Ic","colab_type":"code","outputId":"81bb4ee6-4a21-4adf-d1f5-77e7889366ba","executionInfo":{"status":"ok","timestamp":1589147726797,"user_tz":-180,"elapsed":26182,"user":{"displayName":"Семен Сорокин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOIUUWk_YzzzBBifN_cm14_KVcOtDV3Fuy_9lY=s64","userId":"15238420249980587427"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd drive/'My Drive'/UniversalEmb"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/UniversalEmb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HtL4epXFaJXR","colab_type":"code","outputId":"2b4f02a1-e2a5-461f-8e09-e67bc1193fea","executionInfo":{"status":"ok","timestamp":1589147734094,"user_tz":-180,"elapsed":16283,"user":{"displayName":"Семен Сорокин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOIUUWk_YzzzBBifN_cm14_KVcOtDV3Fuy_9lY=s64","userId":"15238420249980587427"}},"colab":{"base_uri":"https://localhost:8080/","height":561}},"source":["!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n","\r\u001b[K     |▌                               | 10kB 6.2MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 5.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 7.2MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 8.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 8.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 102kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 112kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 122kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 133kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 153kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 163kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 174kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 184kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 194kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 204kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 215kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 225kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 235kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 245kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 256kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 266kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 276kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 286kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 296kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 307kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 317kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 327kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 337kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 348kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 358kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 368kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 378kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 389kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 399kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 409kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 419kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 430kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 440kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 450kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 460kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 471kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 481kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 491kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 501kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 512kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 522kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 532kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 542kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 552kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 563kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 573kB 8.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 583kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 593kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 604kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 614kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 624kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 8.1MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 23.4MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 61.1MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 59.2MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=42504486d54c026c899996af2b10ee06c77b2d5242794d9eb945cfd27330736f\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.7.0 transformers-2.9.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eZn1gtuHaV1M","colab_type":"code","colab":{}},"source":["import pandas as pd\n","def read_trec50(file_name, typ=1):\n","  train = pd.read_csv(file_name, index_col=0)\n","  lbl2indx = {i:j for j, i in enumerate(train.labels.unique())}\n","  train.columns = ['text', 'labels']\n","  train['lbl_index'] = train.labels.apply(lambda x:lbl2indx[x])\n","\n","\n","  labels_t = [i.split(' ', 1)[0].split(':')[typ] for i in open('TREC_10.label', encoding = 'windows-1252').readlines()]\n","  texts_t = [ i.split(' ', 1)[1][:-1] for i in open('TREC_10.label', encoding = 'windows-1252').readlines()]\n","  test  = pd.DataFrame({'text':texts_t, 'labels': labels_t})\n","  test['lbl_index'] = test.labels.apply(lambda x:lbl2indx[x])\n","\n","  return train, test, lbl2indx\n","\n","train, test, l2i = read_trec50('trec50_splited/trec50_3k.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PWM6iToSKRGL","colab_type":"code","colab":{}},"source":["import pandas as pd\n","def read_trec6(file_name, typ=0):\n","  train = pd.read_csv(file_name, index_col=0)\n","  lbl2indx = {i:j for j, i in enumerate(train.labels.unique())}\n","  train.columns = ['text', 'labels']\n","  train['lbl_index'] = train.labels.apply(lambda x:lbl2indx[x])\n","\n","\n","  labels_t = [i.split(' ', 1)[0].split(':')[typ] for i in open('TREC_10.label', encoding = 'windows-1252').readlines()]\n","  texts_t = [ i.split(' ', 1)[1][:-1] for i in open('TREC_10.label', encoding = 'windows-1252').readlines()]\n","  test  = pd.DataFrame({'text':texts_t, 'labels': labels_t})\n","  test['lbl_index'] = test.labels.apply(lambda x:lbl2indx[x])\n","\n","  return train, test, lbl2indx\n","train, test, l2i = read_trec6('trec6_splited/trec6_splited_3k.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fi3oX4vClFih","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cKujnSGGigqt","colab":{}},"source":["import pandas as pd\n","train = pd.read_csv(\"sst2-splited/sst2_full_rev_3k.csv\", index_col=0)\n","train.columns = ['text', 'lbl_index']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"td2kBIa7igqv","colab":{}},"source":["test = pd.read_csv(\"sst2-splited/test.csv\", sep = '\\t')\n","test.columns = ['text', 'lbl_index']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8YXKGOThan2v","colab_type":"code","colab":{}},"source":["import torch\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bBdb3pt8LuQ","colab_type":"code","outputId":"2af307b0-dc9a-4f1e-e662-cfa0bfd6886f","executionInfo":{"status":"ok","timestamp":1589148357507,"user_tz":-180,"elapsed":3093,"user":{"displayName":"Семен Сорокин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOIUUWk_YzzzBBifN_cm14_KVcOtDV3Fuy_9lY=s64","userId":"15238420249980587427"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from keras.preprocessing.sequence import pad_sequences\n","from tqdm import tqdm\n","def prep_data(sentences, MAX_LEN = 64):\n","  input_ids = []\n","  for sent in tqdm(sentences):\n","      encoded_sent = tokenizer.encode(sent, add_special_tokens = True)\n","      input_ids.append(encoded_sent[:MAX_LEN])\n","  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n","  attention_masks = []\n","  for sent in input_ids:\n","      att_mask = [int(token_id > 0) for token_id in sent]\n","      attention_masks.append(att_mask)\n","  return input_ids, attention_masks\n","\n","train_input_ids, train_attention_masks = prep_data(train.text.values.tolist())\n","test_input_ids, test_attention_masks = prep_data(test.text.values.tolist())"],"execution_count":43,"outputs":[{"output_type":"stream","text":["100%|██████████| 3000/3000 [00:00<00:00, 3288.41it/s]\n","100%|██████████| 1821/1821 [00:00<00:00, 3347.99it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"GEgLpFVlo1Z-","colab_type":"code","colab":{}},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","train_inputs = torch.tensor(train_input_ids)\n","validation_inputs = torch.tensor(test_input_ids)\n","\n","train_labels = torch.tensor(train.lbl_index.values.tolist())\n","validation_labels = torch.tensor(test.lbl_index.values.tolist())\n","\n","train_masks = torch.tensor(train_attention_masks)\n","validation_masks = torch.tensor(test_attention_masks)\n","\n","batch_size = 16\n","\n","# Create the DataLoader for our training set.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gFsCTp_mporB","colab_type":"code","colab":{}},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = len(list(train.lbl_index.unique())), # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = True # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model = model.cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8PIiVlDYCtSq","colab_type":"code","colab":{}},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-p0upAhhRiIx","colab_type":"code","colab":{}},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 3\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 50, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9cQNvaZ9bnyy","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gpt6tR83keZD","colab_type":"code","colab":{}},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n","device = 'cuda'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D0UDGOdMtp0M","colab_type":"code","colab":{}},"source":["import keras.backend as K\n","def f1(y_true, y_pred): #taken from old keras source code\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","    return f1_val"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1lJBQY6Ye-73","colab_type":"code","outputId":"a5bf7ed1-22af-4b33-c872-45eeaf8c0fba","executionInfo":{"status":"ok","timestamp":1589148505279,"user_tz":-180,"elapsed":150816,"user":{"displayName":"Семен Сорокин","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOIUUWk_YzzzBBifN_cm14_KVcOtDV3Fuy_9lY=s64","userId":"15238420249980587427"}},"colab":{"base_uri":"https://localhost:8080/","height":816}},"source":["import random\n","from sklearn.metrics import f1_score\n","\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","loss_values = []\n","\n","for epoch_i in range(0, epochs):\n","    embeddings = []\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","    t0 = time.time()\n","    total_loss = 0\n","    model.train()\n","\n","    for step, batch in enumerate(train_dataloader):\n","        if step % 40 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        model.zero_grad()        \n","\n","        outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        loss  = outputs[0]\n","        embeddings.append(outputs[2][-1][:,0,:])\n","        total_loss += loss.item()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","    test_targets, test_pred_class = [], []\n","\n","    model.eval()\n","\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    for batch in validation_dataloader:\n","        \n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():        \n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        logits = outputs[0]\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        test_targets.append(label_ids)\n","        test_pred_class.append(np.argmax(logits, axis=1))\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        nb_eval_steps += 1\n","    test_targets = np.concatenate(test_targets).squeeze()\n","    test_pred_class = np.concatenate(test_pred_class).squeeze()\n","\n","    f1 = f1_score(test_targets, test_pred_class, average='weighted')\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  F1: {0:.4f}\".format(f1))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":51,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 3 ========\n","Training...\n","  Batch    40  of    188.    Elapsed: 0:00:08.\n","  Batch    80  of    188.    Elapsed: 0:00:17.\n","  Batch   120  of    188.    Elapsed: 0:00:26.\n","  Batch   160  of    188.    Elapsed: 0:00:35.\n","\n","  Average training loss: 0.48\n","  Training epcoh took: 0:00:41\n","\n","Running Validation...\n","  Accuracy: 0.89\n","  F1: 0.8851\n","  Validation took: 0:00:08\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","  Batch    40  of    188.    Elapsed: 0:00:08.\n","  Batch    80  of    188.    Elapsed: 0:00:17.\n","  Batch   120  of    188.    Elapsed: 0:00:25.\n","  Batch   160  of    188.    Elapsed: 0:00:34.\n","\n","  Average training loss: 0.20\n","  Training epcoh took: 0:00:40\n","\n","Running Validation...\n","  Accuracy: 0.89\n","  F1: 0.8901\n","  Validation took: 0:00:08\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","  Batch    40  of    188.    Elapsed: 0:00:09.\n","  Batch    80  of    188.    Elapsed: 0:00:17.\n","  Batch   120  of    188.    Elapsed: 0:00:26.\n","  Batch   160  of    188.    Elapsed: 0:00:34.\n","\n","  Average training loss: 0.09\n","  Training epcoh took: 0:00:40\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  F1: 0.8962\n","  Validation took: 0:00:08\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PvUHe-KZpiCR","colab_type":"code","colab":{}},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from tqdm import tqdm\n","\n","def getBertVectors(model, sentences, file_name, batch_size = 64, device = 'cuda'):\n","  input_ids, attention_masks = prep_data(sentences)\n","\n","  inputs = torch.tensor(input_ids)\n","  labels = torch.tensor([0]*len(sentences))\n","  masks = torch.tensor(attention_masks)\n","\n","  train_data = TensorDataset(inputs, masks, labels)\n","  train_sampler = RandomSampler(train_data)\n","  dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","  embeddings = []\n","  model.eval()\n","\n","  for batch in tqdm(dataloader):\n","      b_input_ids = batch[0].to(device)\n","      b_input_mask = batch[1].to(device)\n","      b_labels = batch[2].to(device)\n","      model.zero_grad()        \n","\n","      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","      embeddings.append(outputs[2][-1][:,0,:].detach().cpu().numpy())\n","\n","  embeddings = np.concatenate(embeddings, axis = 0)\n","  np.save(file_name, embeddings)\n","  return embeddings\n"],"execution_count":0,"outputs":[]}]}